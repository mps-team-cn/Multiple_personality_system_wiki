#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä»è¯æ¡ Frontmatter æå–æ ‡é¢˜å’Œ boost æƒé‡,ç”Ÿæˆ jieba user_dict.txt

ç”¨æ³•:
    python3 tools/generate_user_dict_from_entries.py \\
        --entries-dir docs/entries \\
        --output data/user_dict.txt \\
        --base-freq 1000
"""

import re
import argparse
from pathlib import Path
from typing import Dict, List, Tuple
import yaml
import sys


def extract_frontmatter(file_path: Path) -> Dict:
    """ä» Markdown æ–‡ä»¶ä¸­æå– Frontmatter"""
    if not file_path.exists():
        return {}

    content = file_path.read_text(encoding='utf-8')
    match = re.match(r'^---\s*\n(.*?)\n---\s*\n', content, re.DOTALL)
    if not match:
        return {}

    try:
        return yaml.safe_load(match.group(1)) or {}
    except Exception as e:
        print(f"è­¦å‘Š: æ— æ³•è§£æ {file_path} çš„ Frontmatter: {e}", file=sys.stderr)
        return {}


def extract_title_from_content(content: str) -> str | None:
    """ä» Markdown å†…å®¹ä¸­æå–ä¸€çº§æ ‡é¢˜"""
    # è·³è¿‡ Frontmatter
    content_without_fm = re.sub(r'^---\s*\n.*?\n---\s*\n', '', content, flags=re.DOTALL)

    # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªä¸€çº§æ ‡é¢˜
    match = re.search(r'^#\s+(.+)$', content_without_fm, re.MULTILINE)
    if match:
        title = match.group(1).strip()
        # æå–ä¸­æ–‡éƒ¨åˆ† (æ ¼å¼: ä¸­æ–‡å(English/ç¼©å†™))
        cn_match = re.match(r'^([^(ï¼ˆ]+)', title)
        if cn_match:
            return cn_match.group(1).strip()
    return None


def calculate_frequency(boost: float, base_freq: int = 1000) -> int:
    """æ ¹æ® boost æƒé‡è®¡ç®—è¯é¢‘

    Args:
        boost: æƒé‡å€¼ (0.1-3.0)
        base_freq: åŸºç¡€é¢‘ç‡

    Returns:
        è®¡ç®—åçš„è¯é¢‘
    """
    # boost è½¬æ¢ä¸ºè¯é¢‘çš„æ˜ å°„:
    # boost=2.0 â†’ 9999 (æœ€é«˜ä¼˜å…ˆçº§)
    # boost=1.8 â†’ 8000
    # boost=1.5 â†’ 7000
    # boost=1.2 â†’ 5000
    # boost=1.0 â†’ 3000
    # boost<1.0 â†’ åŸºäº base_freq è®¡ç®—

    if boost >= 2.0:
        return 9999
    elif boost >= 1.8:
        return 8000
    elif boost >= 1.5:
        return 7000
    elif boost >= 1.3:
        return 6000
    elif boost >= 1.2:
        return 5000
    elif boost >= 1.0:
        return 3000
    else:
        return int(base_freq * boost)


def extract_entries_with_boost(entries_dir: Path, base_freq: int = 1000) -> List[Tuple[str, int]]:
    """ä»è¯æ¡ç›®å½•æå–æ‰€æœ‰è¯æ¡æ ‡é¢˜å’Œæƒé‡

    Returns:
        List of (è¯æ¡, è¯é¢‘) tuples
    """
    entries = []

    for md_file in entries_dir.glob('*.md'):
        # è·³è¿‡ç´¢å¼•å’Œå¯¼è§ˆæ–‡ä»¶
        if md_file.stem.endswith('-index') or md_file.stem.endswith('-Guide'):
            continue

        frontmatter = extract_frontmatter(md_file)

        # è·å–æ ‡é¢˜
        title = frontmatter.get('title')
        if not title:
            # ä»å†…å®¹ä¸­æå–æ ‡é¢˜
            content = md_file.read_text(encoding='utf-8')
            title = extract_title_from_content(content)

        if not title:
            print(f"è­¦å‘Š: {md_file.name} æ²¡æœ‰æ‰¾åˆ°æ ‡é¢˜", file=sys.stderr)
            continue

        # è·å– boost æƒé‡ (å¯èƒ½åœ¨ search.boost æˆ–ç›´æ¥åœ¨ frontmatter.boost)
        boost = None
        search_config = frontmatter.get('search', {})
        if isinstance(search_config, dict):
            boost = search_config.get('boost')
        if boost is None:
            boost = frontmatter.get('boost')

        if boost is not None:
            try:
                boost_value = float(boost)
                freq = calculate_frequency(boost_value, base_freq)
            except (ValueError, TypeError):
                print(f"è­¦å‘Š: {md_file.name} çš„ boost å€¼æ— æ•ˆ: {boost}", file=sys.stderr)
                freq = base_freq
        else:
            # æ²¡æœ‰ boost çš„è¯æ¡ä½¿ç”¨åŸºç¡€é¢‘ç‡
            freq = base_freq

        entries.append((title, freq))

        # åŒæ—¶æå– synonyms ä½œä¸ºé¢å¤–è¯æ¡
        synonyms = frontmatter.get('synonyms', [])
        if isinstance(synonyms, str):
            synonyms = [s.strip() for s in synonyms.split(',') if s.strip()]

        for syn in synonyms:
            if syn and syn != title:
                # åŒä¹‰è¯ä½¿ç”¨ç›¸åŒçš„é¢‘ç‡
                entries.append((syn, freq))

    return entries


def extract_common_terms(entries: List[Tuple[str, int]]) -> List[Tuple[str, int]]:
    """ä»è¯æ¡ä¸­æå–å¸¸è§æœ¯è¯­å’Œå¤åˆè¯

    æå–è§„åˆ™:
    - åŒ…å«"éšœç¢"ã€"ç—‡"ã€"ç†è®º"ç­‰åç¼€çš„è¯
    - å¸¸è§çš„ä¸“ä¸šæœ¯è¯­
    """
    terms = set()

    for title, freq in entries:
        # æå–åŒ…å«ç‰¹å®šåç¼€çš„å­è¯
        # ä¾‹å¦‚: "è§£ç¦»æ€§èº«ä»½éšœç¢" â†’ "èº«ä»½éšœç¢"
        if 'éšœç¢' in title and len(title) > 2:
            # æå– XXéšœç¢
            match = re.search(r'(.{1,4}éšœç¢)', title)
            if match:
                term = match.group(1)
                if len(term) >= 2:
                    terms.add((term, max(2000, freq // 2)))

        # å…¶ä»–ç±»ä¼¼æ¨¡å¼å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ 

    return list(terms)


def generate_user_dict(entries: List[Tuple[str, int]], output_path: Path, include_terms: bool = True):
    """ç”Ÿæˆ jieba user_dict.txt

    Args:
        entries: List of (è¯æ¡, è¯é¢‘) tuples
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        include_terms: æ˜¯å¦åŒ…å«æå–çš„é€šç”¨æœ¯è¯­
    """
    # å»é‡å¹¶æŒ‰é¢‘ç‡æ’åº
    entry_dict = {}
    for word, freq in entries:
        if word in entry_dict:
            # ä¿ç•™æ›´é«˜çš„é¢‘ç‡
            entry_dict[word] = max(entry_dict[word], freq)
        else:
            entry_dict[word] = freq

    # å¦‚æœéœ€è¦,æ·»åŠ æå–çš„é€šç”¨æœ¯è¯­
    if include_terms:
        terms = extract_common_terms(entries)
        for term, freq in terms:
            if term not in entry_dict:
                entry_dict[term] = freq

    # æŒ‰é¢‘ç‡é™åºæ’åº
    sorted_entries = sorted(entry_dict.items(), key=lambda x: (-x[1], x[0]))

    # å†™å…¥æ–‡ä»¶
    output_path.parent.mkdir(parents=True, exist_ok=True)

    with open(output_path, 'w', encoding='utf-8') as f:
        f.write("# Generated jieba userdict for MPS + medical domain\n")
        f.write("# æ ¼å¼: è¯è¯­ è¯é¢‘ è¯æ€§\n")

        for word, freq in sorted_entries:
            # jieba æ ¼å¼: è¯è¯­ è¯é¢‘ è¯æ€§
            # è¯æ€§è®¾ä¸º n (åè¯)
            f.write(f"{word} {freq} n\n")

    print(f"\nâœ… ç”Ÿæˆå®Œæˆ: {output_path}")
    print(f"   æ€»è¯æ¡æ•°: {len(sorted_entries)}")
    print(f"   æœ€é«˜é¢‘ç‡: {sorted_entries[0][1]}")
    print(f"   æœ€ä½é¢‘ç‡: {sorted_entries[-1][1]}")


def print_preview(entries: List[Tuple[str, int]], top_n: int = 30):
    """æ‰“å°é¢„è§ˆ"""
    print("\n" + "="*70)
    print(f"è¯æ¡é¢„è§ˆ (Top {top_n})")
    print("="*70)

    # å»é‡
    entry_dict = {}
    for word, freq in entries:
        if word in entry_dict:
            entry_dict[word] = max(entry_dict[word], freq)
        else:
            entry_dict[word] = freq

    sorted_entries = sorted(entry_dict.items(), key=lambda x: (-x[1], x[0]))[:top_n]

    for i, (word, freq) in enumerate(sorted_entries, 1):
        print(f"{i:3d}. {word:20s} {freq:6,}")

    print("="*70)


def main():
    parser = argparse.ArgumentParser(
        description='ä»è¯æ¡ Frontmatter æå–æ ‡é¢˜å’Œ boost æƒé‡,ç”Ÿæˆ jieba user_dict.txt'
    )
    parser.add_argument(
        '--entries-dir',
        default='docs/entries',
        help='è¯æ¡ç›®å½•è·¯å¾„ (é»˜è®¤: docs/entries)'
    )
    parser.add_argument(
        '--output',
        default='data/user_dict.txt',
        help='è¾“å‡ºè¯å…¸æ–‡ä»¶è·¯å¾„ (é»˜è®¤: data/user_dict.txt)'
    )
    parser.add_argument(
        '--base-freq',
        type=int,
        default=1000,
        help='åŸºç¡€è¯é¢‘ (é»˜è®¤: 1000)'
    )
    parser.add_argument(
        '--no-terms',
        action='store_true',
        help='ä¸åŒ…å«è‡ªåŠ¨æå–çš„é€šç”¨æœ¯è¯­'
    )
    parser.add_argument(
        '--preview',
        type=int,
        default=30,
        help='é¢„è§ˆæ˜¾ç¤ºçš„è¯æ•° (é»˜è®¤: 30)'
    )

    args = parser.parse_args()

    entries_dir = Path(args.entries_dir)
    if not entries_dir.exists():
        print(f"é”™è¯¯: è¯æ¡ç›®å½•ä¸å­˜åœ¨: {entries_dir}", file=sys.stderr)
        sys.exit(1)

    print(f"ğŸ“– æ­£åœ¨æ‰«æè¯æ¡ç›®å½•: {entries_dir}")

    # æå–è¯æ¡
    entries = extract_entries_with_boost(entries_dir, args.base_freq)

    print(f"âœ… æå–äº† {len(entries)} ä¸ªè¯æ¡(å«åŒä¹‰è¯)")

    # æ˜¾ç¤ºé¢„è§ˆ
    if args.preview > 0:
        print_preview(entries, args.preview)

    # ç”Ÿæˆè¯å…¸
    output_path = Path(args.output)
    generate_user_dict(entries, output_path, include_terms=not args.no_terms)

    print("\nâœ… å®Œæˆ!")


if __name__ == '__main__':
    main()
