#!/usr/bin/env python3
"""
Google Indexing API è‡ªåŠ¨æäº¤å·¥å…·
ä½¿ç”¨ Google Indexing API æ‰¹é‡æäº¤ URL åˆ° Google Search Console ä»¥åŠ é€Ÿç´¢å¼•

åŠŸèƒ½ç‰¹æ€§:
- è¯»å–ç”± generate_seo_urls.py ç”Ÿæˆçš„ URL åˆ—è¡¨
- æ”¯æŒæ‰¹é‡æäº¤(æœ€å¤š 100 ä¸ª URL/æ‰¹æ¬¡)
- Service Account JSON è®¤è¯
- æ™ºèƒ½é…é¢ç®¡ç†(é»˜è®¤ 200 è¯·æ±‚/å¤©)
- é‡è¯•æœºåˆ¶å¤„ç†ä¸´æ—¶é”™è¯¯
- è¯¦ç»†çš„æ—¥å¿—è¾“å‡º
- Dry-run æ¨¡å¼ç”¨äºæµ‹è¯•

ä½¿ç”¨ç¤ºä¾‹:
    # ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„ Service Account JSON
    python3 tools/submit_to_google_indexing.py

    # æŒ‡å®š JSON æ–‡ä»¶è·¯å¾„
    python3 tools/submit_to_google_indexing.py --credentials /path/to/service-account.json

    # Dry-run æ¨¡å¼
    python3 tools/submit_to_google_indexing.py --dry-run

    # æŒ‡å®šä¼˜å…ˆçº§(1=æœ€é«˜,2=é«˜)
    python3 tools/submit_to_google_indexing.py --max-priority 2

    # é™åˆ¶æäº¤æ•°é‡
    python3 tools/submit_to_google_indexing.py --limit 50

    # è‡ªåŠ¨ç¡®è®¤æäº¤(ç”¨äºè‡ªåŠ¨åŒ–åœºæ™¯,è·³è¿‡äº¤äº’å¼ç¡®è®¤)
    python3 tools/submit_to_google_indexing.py --yes
"""

import argparse
import json
import logging
import os
import sys
import time
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from datetime import datetime

try:
    from google.oauth2 import service_account
    from googleapiclient.discovery import build
    from googleapiclient.errors import HttpError
except ImportError:
    print("é”™è¯¯: ç¼ºå°‘å¿…éœ€çš„ Google API åº“")
    print("è¯·è¿è¡Œ: pip install google-auth google-api-python-client")
    sys.exit(1)

# å¯¼å…¥ URL ç”Ÿæˆå™¨
try:
    from generate_seo_urls import generate_url_list, BASE_URL
except ImportError:
    print("é”™è¯¯: æ— æ³•å¯¼å…¥ generate_seo_urls æ¨¡å—")
    print("è¯·ç¡®ä¿ generate_seo_urls.py åœ¨åŒä¸€ç›®å½•ä¸‹")
    sys.exit(1)

# Google Indexing API é…ç½®
SCOPES = ["https://www.googleapis.com/auth/indexing"]
API_SERVICE_NAME = "indexing"
API_VERSION = "v3"
BATCH_SIZE = 100  # Google API æ‰¹é‡è¯·æ±‚é™åˆ¶
DAILY_QUOTA = 200  # é»˜è®¤é…é¢
RETRY_ATTEMPTS = 3
RETRY_DELAY = 2  # ç§’

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)
logger = logging.getLogger(__name__)


class GoogleIndexingClient:
    """Google Indexing API å®¢æˆ·ç«¯"""

    def __init__(self, credentials_path: Optional[str] = None, dry_run: bool = False):
        """
        åˆå§‹åŒ–å®¢æˆ·ç«¯

        Args:
            credentials_path: Service Account JSON æ–‡ä»¶è·¯å¾„
            dry_run: æ˜¯å¦ä¸º dry-run æ¨¡å¼
        """
        self.dry_run = dry_run
        self.service = None
        self.submitted_count = 0
        self.failed_count = 0
        self.skipped_count = 0

        if not dry_run:
            self._initialize_service(credentials_path)

    def _initialize_service(self, credentials_path: Optional[str] = None):
        """åˆå§‹åŒ– Google API æœåŠ¡"""
        try:
            # ä»æ–‡ä»¶æˆ–ç¯å¢ƒå˜é‡è·å–å‡­è¯
            if credentials_path:
                logger.info(f"ä»æ–‡ä»¶åŠ è½½å‡­è¯: {credentials_path}")
                credentials = service_account.Credentials.from_service_account_file(
                    credentials_path, scopes=SCOPES
                )
            else:
                # å°è¯•ä»ç¯å¢ƒå˜é‡åŠ è½½
                creds_json = os.environ.get("GOOGLE_SERVICE_ACCOUNT_JSON")
                if not creds_json:
                    raise ValueError(
                        "æœªæä¾›å‡­è¯: è¯·é€šè¿‡ --credentials å‚æ•°æŒ‡å®šæ–‡ä»¶è·¯å¾„,æˆ–è®¾ç½® "
                        "GOOGLE_SERVICE_ACCOUNT_JSON ç¯å¢ƒå˜é‡"
                    )

                logger.info("ä»ç¯å¢ƒå˜é‡ GOOGLE_SERVICE_ACCOUNT_JSON åŠ è½½å‡­è¯")
                creds_info = json.loads(creds_json)
                credentials = service_account.Credentials.from_service_account_info(
                    creds_info, scopes=SCOPES
                )

            # æ„å»º API æœåŠ¡
            self.service = build(API_SERVICE_NAME, API_VERSION, credentials=credentials)
            logger.info("Google Indexing API æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")

        except FileNotFoundError:
            logger.error(f"å‡­è¯æ–‡ä»¶ä¸å­˜åœ¨: {credentials_path}")
            raise
        except json.JSONDecodeError as e:
            logger.error(f"JSON æ ¼å¼é”™è¯¯: {e}")
            raise
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    def submit_url(self, url: str, url_type: str = "URL_UPDATED") -> bool:
        """
        æäº¤å•ä¸ª URL

        Args:
            url: è¦æäº¤çš„ URL
            url_type: è¯·æ±‚ç±»å‹ (URL_UPDATED æˆ– URL_DELETED)

        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        if self.dry_run:
            logger.info(f"[DRY-RUN] å°†æäº¤: {url}")
            return True

        for attempt in range(1, RETRY_ATTEMPTS + 1):
            try:
                body = {
                    "url": url,
                    "type": url_type
                }

                response = self.service.urlNotifications().publish(body=body).execute()
                logger.info(f"âœ“ æˆåŠŸæäº¤: {url}")
                self.submitted_count += 1
                return True

            except HttpError as e:
                error_details = e.error_details if hasattr(e, 'error_details') else str(e)

                # å¤„ç†ç‰¹å®šé”™è¯¯
                if e.resp.status == 403:
                    logger.error(f"âœ— æƒé™ä¸è¶³ (403): {url}")
                    logger.error("  æç¤º: è¯·ç¡®è®¤ Service Account å·²è¢«æ·»åŠ ä¸ºç½‘ç«™æ‰€æœ‰è€…")
                    self.failed_count += 1
                    return False
                elif e.resp.status == 429:
                    logger.warning(f"âš  é…é¢è¶…é™ (429): {url}")
                    if attempt < RETRY_ATTEMPTS:
                        delay = RETRY_DELAY * attempt
                        logger.info(f"  ç­‰å¾… {delay} ç§’åé‡è¯• (å°è¯• {attempt}/{RETRY_ATTEMPTS})")
                        time.sleep(delay)
                        continue
                    else:
                        logger.error(f"âœ— è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°: {url}")
                        self.failed_count += 1
                        return False
                elif e.resp.status == 400:
                    logger.error(f"âœ— è¯·æ±‚æ— æ•ˆ (400): {url}")
                    logger.error(f"  è¯¦æƒ…: {error_details}")
                    self.failed_count += 1
                    return False
                else:
                    logger.error(f"âœ— HTTP é”™è¯¯ ({e.resp.status}): {url}")
                    logger.error(f"  è¯¦æƒ…: {error_details}")
                    if attempt < RETRY_ATTEMPTS:
                        delay = RETRY_DELAY * attempt
                        logger.info(f"  ç­‰å¾… {delay} ç§’åé‡è¯• (å°è¯• {attempt}/{RETRY_ATTEMPTS})")
                        time.sleep(delay)
                        continue
                    self.failed_count += 1
                    return False

            except Exception as e:
                logger.error(f"âœ— æäº¤å¤±è´¥: {url}")
                logger.error(f"  é”™è¯¯: {type(e).__name__}: {e}")
                if attempt < RETRY_ATTEMPTS:
                    delay = RETRY_DELAY * attempt
                    logger.info(f"  ç­‰å¾… {delay} ç§’åé‡è¯• (å°è¯• {attempt}/{RETRY_ATTEMPTS})")
                    time.sleep(delay)
                    continue
                self.failed_count += 1
                return False

        return False

    def submit_batch(self, urls: List[str]) -> Dict[str, int]:
        """
        æ‰¹é‡æäº¤ URL

        Args:
            urls: URL åˆ—è¡¨

        Returns:
            dict: ç»Ÿè®¡ä¿¡æ¯ {submitted, failed, skipped}
        """
        total = len(urls)
        logger.info(f"å¼€å§‹æ‰¹é‡æäº¤ {total} ä¸ª URL")

        for i, url in enumerate(urls, 1):
            logger.info(f"è¿›åº¦: [{i}/{total}]")

            # æ£€æŸ¥é…é¢é™åˆ¶
            if not self.dry_run and self.submitted_count >= DAILY_QUOTA:
                logger.warning(f"âš  è¾¾åˆ°æ¯æ—¥é…é¢é™åˆ¶ ({DAILY_QUOTA} ä¸ªè¯·æ±‚)")
                self.skipped_count += total - i + 1
                break

            self.submit_url(url)

            # é¿å…è§¦å‘é€Ÿç‡é™åˆ¶
            if not self.dry_run and i < total:
                time.sleep(0.5)

        return {
            "submitted": self.submitted_count,
            "failed": self.failed_count,
            "skipped": self.skipped_count
        }

    def get_metadata(self, url: str) -> Optional[Dict]:
        """
        æŸ¥è¯¢ URL çš„ç´¢å¼•çŠ¶æ€

        Args:
            url: è¦æŸ¥è¯¢çš„ URL

        Returns:
            dict: å…ƒæ•°æ®ä¿¡æ¯,å¤±è´¥è¿”å› None
        """
        if self.dry_run:
            logger.info(f"[DRY-RUN] å°†æŸ¥è¯¢: {url}")
            return None

        try:
            response = self.service.urlNotifications().getMetadata(url=url).execute()
            return response
        except HttpError as e:
            if e.resp.status == 404:
                logger.info(f"URL æœªè¢«ç´¢å¼•: {url}")
            else:
                logger.error(f"æŸ¥è¯¢å¤±è´¥ ({e.resp.status}): {url}")
            return None
        except Exception as e:
            logger.error(f"æŸ¥è¯¢å¤±è´¥: {type(e).__name__}: {e}")
            return None


def load_url_list(max_priority: int = 5, limit: Optional[int] = None) -> List[Tuple[int, str, str]]:
    """
    åŠ è½½ URL åˆ—è¡¨

    Args:
        max_priority: æœ€å¤§ä¼˜å…ˆçº§(1-5,1 ä¸ºæœ€é«˜)
        limit: é™åˆ¶æ•°é‡

    Returns:
        list: [(ä¼˜å…ˆçº§, URL, æè¿°)]
    """
    logger.info("ç”Ÿæˆ URL åˆ—è¡¨...")
    urls = generate_url_list()

    # æŒ‰ä¼˜å…ˆçº§è¿‡æ»¤
    urls = [u for u in urls if u[0] <= max_priority]
    logger.info(f"ä¼˜å…ˆçº§ <= {max_priority} çš„ URL: {len(urls)} ä¸ª")

    # æŒ‰ä¼˜å…ˆçº§æ’åº(ä¼˜å…ˆçº§ä½çš„æ•°å­—å…ˆæäº¤)
    urls.sort(key=lambda x: (x[0], x[1]))

    # é™åˆ¶æ•°é‡
    if limit and limit > 0:
        urls = urls[:limit]
        logger.info(f"é™åˆ¶æäº¤æ•°é‡ä¸º: {limit} ä¸ª")

    return urls


def print_summary(stats: Dict[str, int], start_time: float):
    """æ‰“å°æäº¤æ‘˜è¦"""
    elapsed = time.time() - start_time
    total = stats["submitted"] + stats["failed"] + stats["skipped"]

    print("\n" + "=" * 80)
    print("æäº¤æ‘˜è¦")
    print("=" * 80)
    print(f"æ€»è®¡:   {total} ä¸ª URL")
    print(f"æˆåŠŸ:   {stats['submitted']} ä¸ª")
    print(f"å¤±è´¥:   {stats['failed']} ä¸ª")
    print(f"è·³è¿‡:   {stats['skipped']} ä¸ª")
    print(f"ç”¨æ—¶:   {elapsed:.2f} ç§’")
    print("=" * 80)

    if stats["failed"] > 0:
        print("\nâš  éƒ¨åˆ† URL æäº¤å¤±è´¥,è¯·æŸ¥çœ‹ä¸Šæ–¹æ—¥å¿—äº†è§£è¯¦æƒ…")

    if stats["skipped"] > 0:
        print(f"\nğŸ’¡ æœ‰ {stats['skipped']} ä¸ª URL å› é…é¢é™åˆ¶æœªæäº¤")
        print("   å»ºè®®æ˜å¤©ç»§ç»­æäº¤å‰©ä½™ URL")


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="Google Indexing API è‡ªåŠ¨æäº¤å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„å‡­è¯
  python3 %(prog)s

  # æŒ‡å®šå‡­è¯æ–‡ä»¶
  python3 %(prog)s --credentials /path/to/service-account.json

  # Dry-run æ¨¡å¼
  python3 %(prog)s --dry-run

  # åªæäº¤æœ€é«˜ä¼˜å…ˆçº§çš„ URL
  python3 %(prog)s --max-priority 1

  # é™åˆ¶æäº¤æ•°é‡
  python3 %(prog)s --limit 50

  # è‡ªåŠ¨ç¡®è®¤æäº¤(ç”¨äº CI/CD æˆ–å®šæ—¶ä»»åŠ¡)
  python3 %(prog)s --yes

  # ç»„åˆä½¿ç”¨
  python3 %(prog)s --max-priority 2 --limit 100 --yes

ç¯å¢ƒå˜é‡:
  GOOGLE_SERVICE_ACCOUNT_JSON  Service Account JSON å†…å®¹
        """
    )

    parser.add_argument(
        "--credentials",
        "-c",
        type=str,
        help="Service Account JSON æ–‡ä»¶è·¯å¾„(å¯é€‰,é»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–)"
    )

    parser.add_argument(
        "--max-priority",
        "-p",
        type=int,
        default=5,
        choices=[1, 2, 3, 4, 5],
        help="æœ€å¤§ä¼˜å…ˆçº§ (1=æœ€é«˜, 5=æœ€ä½, é»˜è®¤: 5)"
    )

    parser.add_argument(
        "--limit",
        "-l",
        type=int,
        help="é™åˆ¶æäº¤æ•°é‡(å¯é€‰)"
    )

    parser.add_argument(
        "--dry-run",
        "-d",
        action="store_true",
        help="Dry-run æ¨¡å¼,ä¸å®é™…æäº¤"
    )

    parser.add_argument(
        "--query",
        "-q",
        type=str,
        help="æŸ¥è¯¢æŒ‡å®š URL çš„ç´¢å¼•çŠ¶æ€(ä¸æ‰§è¡Œæäº¤)"
    )

    parser.add_argument(
        "--verbose",
        "-v",
        action="store_true",
        help="æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—"
    )

    parser.add_argument(
        "--yes",
        "-y",
        action="store_true",
        help="è‡ªåŠ¨ç¡®è®¤æäº¤,è·³è¿‡äº¤äº’å¼ç¡®è®¤(ç”¨äºè‡ªåŠ¨åŒ–åœºæ™¯)"
    )

    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()

    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        logger.setLevel(logging.DEBUG)

    print("=" * 80)
    print("Google Indexing API è‡ªåŠ¨æäº¤å·¥å…·")
    print(f"ç½‘ç«™åœ°å€: {BASE_URL}")
    if args.dry_run:
        print("æ¨¡å¼: DRY-RUN (ä¸ä¼šå®é™…æäº¤)")
    print("=" * 80)
    print()

    try:
        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        client = GoogleIndexingClient(
            credentials_path=args.credentials,
            dry_run=args.dry_run
        )

        # æŸ¥è¯¢æ¨¡å¼
        if args.query:
            logger.info(f"æŸ¥è¯¢ URL ç´¢å¼•çŠ¶æ€: {args.query}")
            metadata = client.get_metadata(args.query)
            if metadata:
                print("\nç´¢å¼•å…ƒæ•°æ®:")
                print(json.dumps(metadata, indent=2, ensure_ascii=False))
            else:
                print("\næ— ç´¢å¼•æ•°æ®")
            return

        # åŠ è½½ URL åˆ—è¡¨
        urls = load_url_list(
            max_priority=args.max_priority,
            limit=args.limit
        )

        if not urls:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ URL")
            return

        # æ˜¾ç¤ºå¾…æäº¤çš„ URL
        print(f"\nå¾…æäº¤çš„ URL ({len(urls)} ä¸ª):")
        print("-" * 80)
        for priority, url, desc in urls[:10]:  # åªæ˜¾ç¤ºå‰ 10 ä¸ª
            print(f"[ä¼˜å…ˆçº§ {priority}] {url}")
        if len(urls) > 10:
            print(f"... è¿˜æœ‰ {len(urls) - 10} ä¸ª URL")
        print("-" * 80)
        print()

        # ç¡®è®¤æäº¤(é™¤éä½¿ç”¨äº† --yes å‚æ•°)
        if not args.dry_run and not args.yes:
            response = input("ç¡®è®¤æäº¤? (y/N): ")
            if response.lower() != 'y':
                logger.info("å·²å–æ¶ˆ")
                return

        # æ‰¹é‡æäº¤
        start_time = time.time()
        url_list = [url for _, url, _ in urls]
        stats = client.submit_batch(url_list)

        # æ‰“å°æ‘˜è¦
        print_summary(stats, start_time)

        # ä¿å­˜æäº¤è®°å½•
        if not args.dry_run and stats["submitted"] > 0:
            log_file = Path(__file__).parent.parent / "google_indexing_log.json"
            log_data = {
                "timestamp": datetime.now().isoformat(),
                "stats": stats,
                "submitted_urls": url_list[:stats["submitted"]]
            }

            with open(log_file, "w", encoding="utf-8") as f:
                json.dump(log_data, f, indent=2, ensure_ascii=False)

            logger.info(f"æäº¤è®°å½•å·²ä¿å­˜åˆ°: {log_file}")

    except KeyboardInterrupt:
        logger.warning("\nç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        logger.error(f"æ‰§è¡Œå¤±è´¥: {type(e).__name__}: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
