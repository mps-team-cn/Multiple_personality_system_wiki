#!/usr/bin/env python3
"""
ä»æœç´¢ç´¢å¼•ä¸­æå–æ–‡æœ¬ï¼Œä¾› AI åˆ†æå¹¶ç”Ÿæˆä¼˜åŒ–çš„è¯å…¸

ä½¿ç”¨æ–¹æ³•:
    python3 tools/extract_terms_for_ai.py

è¾“å‡º:
    data/search_index_sample.txt - AI åˆ†æç”¨çš„æ–‡æœ¬æ ·æœ¬
"""

import json
import re
import sys
from pathlib import Path
from collections import Counter

def extract_chinese_terms(text, min_len=2, max_len=6):
    """æå–ä¸­æ–‡è¯ç»„"""
    # åŒ¹é…è¿ç»­çš„ä¸­æ–‡å­—ç¬¦
    pattern = r'[\u4e00-\u9fff]+'
    terms = re.findall(pattern, text)
    # è¿‡æ»¤é•¿åº¦
    return [t for t in terms if min_len <= len(t) <= max_len]

def main():
    print("=== æœç´¢ç´¢å¼•æ–‡æœ¬æå–å·¥å…· ===\n")

    # è¯»å–æœç´¢ç´¢å¼•
    index_path = Path('site/search/search_index.json')
    if not index_path.exists():
        print("âŒ é”™è¯¯: æ‰¾ä¸åˆ°æœç´¢ç´¢å¼•æ–‡ä»¶")
        print("   è¯·å…ˆè¿è¡Œ: mkdocs build")
        sys.exit(1)

    print(f"ğŸ“‚ è¯»å–ç´¢å¼•: {index_path}")
    with open(index_path, 'r', encoding='utf-8') as f:
        index = json.load(f)

    docs = index.get('docs', [])
    print(f"ğŸ“Š æ–‡æ¡£æ€»æ•°: {len(docs)}")

    # æå–æ‰€æœ‰æ–‡æœ¬
    all_texts = []
    all_terms = []

    for i, doc in enumerate(docs):
        title = doc.get('title', '').strip()
        text = doc.get('text', '').strip()
        location = doc.get('location', '')

        # æå–ä¸­æ–‡è¯ç»„
        terms = extract_chinese_terms(title + ' ' + text)
        all_terms.extend(terms)

        # ä¿å­˜æ–‡æ¡£ä¿¡æ¯
        if title and text:
            all_texts.append({
                'id': i,
                'title': title,
                'text': text[:300],  # åªå–å‰300å­—
                'location': location
            })

    print(f"ğŸ“ æœ‰æ•ˆæ–‡æ¡£: {len(all_texts)}")
    print(f"ğŸ”¤ æå–è¯ç»„: {len(all_terms)}")

    # ç»Ÿè®¡é«˜é¢‘è¯
    term_freq = Counter(all_terms)
    print(f"ğŸ“Š ä¸åŒè¯ç»„: {len(term_freq)}")

    # ç”Ÿæˆåˆ†ææŠ¥å‘Š
    output_dir = Path('data')
    output_dir.mkdir(exist_ok=True)

    # 1. å®Œæ•´æ–‡æœ¬æ ·æœ¬ï¼ˆç”¨äº AI ç†è§£å†…å®¹ï¼‰
    sample_file = output_dir / 'search_index_sample.txt'
    with open(sample_file, 'w', encoding='utf-8') as f:
        f.write("# æœç´¢ç´¢å¼•æ–‡æœ¬æ ·æœ¬ï¼ˆä¾› AI åˆ†æï¼‰\n\n")
        f.write(f"æ–‡æ¡£æ€»æ•°: {len(docs)}\n")
        f.write(f"æ ·æœ¬æ•°é‡: {min(100, len(all_texts))}\n\n")
        f.write("="*60 + "\n\n")

        # å†™å…¥å‰100ä¸ªæ–‡æ¡£
        for doc in all_texts[:100]:
            f.write(f"## {doc['title']}\n\n")
            f.write(f"ä½ç½®: {doc['location']}\n\n")
            f.write(f"{doc['text']}\n\n")
            f.write("-"*60 + "\n\n")

    print(f"âœ… å·²ä¿å­˜æ ·æœ¬: {sample_file}")

    # 2. é«˜é¢‘è¯ç»Ÿè®¡ï¼ˆç”¨äº AI è¯†åˆ«éœ€è¦åŠ å…¥è¯å…¸çš„è¯ï¼‰
    freq_file = output_dir / 'term_frequency.txt'
    with open(freq_file, 'w', encoding='utf-8') as f:
        f.write("# é«˜é¢‘ä¸­æ–‡è¯ç»„ç»Ÿè®¡\n\n")
        f.write(f"æ€»è¯ç»„æ•°: {len(all_terms)}\n")
        f.write(f"ä¸åŒè¯ç»„: {len(term_freq)}\n\n")
        f.write("è¯ç»„       é¢‘æ¬¡\n")
        f.write("="*30 + "\n")

        # å†™å…¥ Top 500
        for term, count in term_freq.most_common(500):
            if count >= 3:  # åªä¿ç•™å‡ºç°3æ¬¡ä»¥ä¸Šçš„
                f.write(f"{term:12s} {count:4d}\n")

    print(f"âœ… å·²ä¿å­˜è¯é¢‘: {freq_file}")

    # 3. ç”Ÿæˆ AI æç¤ºè¯
    prompt_file = output_dir / 'ai_prompt.md'
    with open(prompt_file, 'w', encoding='utf-8') as f:
        f.write("# AI è¯å…¸ç”Ÿæˆæç¤ºè¯\n\n")
        f.write("## ä»»åŠ¡è¯´æ˜\n\n")
        f.write("è¯·åˆ†æä»¥ä¸‹æ–‡æ¡£å†…å®¹å’Œé«˜é¢‘è¯ç»„ï¼Œç”Ÿæˆä¸€ä¸ªä¼˜åŒ–çš„ Jieba è‡ªå®šä¹‰è¯å…¸ã€‚\n\n")
        f.write("## è¦æ±‚\n\n")
        f.write("1. è¯†åˆ«ä¸“ä¸šæœ¯è¯­ï¼ˆåŒ»å­¦ã€å¿ƒç†å­¦ã€ç³»ç»Ÿç›¸å…³ï¼‰\n")
        f.write("2. è¯†åˆ«å¤åˆè¯éœ€è¦æ‹†åˆ†çš„åŸºç¡€è¯\n")
        f.write("3. è¯†åˆ«äººåã€ç³»ç»Ÿåç­‰ä¸“æœ‰åè¯\n")
        f.write("4. è¾“å‡ºæ ¼å¼ï¼šè¯è¯­ 999 è¯æ€§\n")
        f.write("5. æŒ‰ç±»åˆ«åˆ†ç»„ï¼ˆä¸´åºŠæœ¯è¯­ã€æ ¸å¿ƒæ¦‚å¿µã€ç†è®ºæ¡†æ¶ç­‰ï¼‰\n\n")
        f.write("## å½“å‰è¯å…¸å‚è€ƒ\n\n")
        f.write("```\n")

        # è¯»å–å½“å‰è¯å…¸
        dict_file = Path('data/user_dict.txt')
        if dict_file.exists():
            with open(dict_file, 'r', encoding='utf-8') as df:
                f.write(df.read()[:2000])  # åªæ˜¾ç¤ºå‰2000å­—

        f.write("\n```\n\n")
        f.write("## é«˜é¢‘è¯ç»„ï¼ˆTop 100ï¼‰\n\n")
        f.write("```\n")
        for term, count in term_freq.most_common(100):
            f.write(f"{term:12s} {count:4d}\n")
        f.write("```\n\n")
        f.write("## æ–‡æ¡£æ ·æœ¬\n\n")
        f.write(f"è¯¦è§: {sample_file.name}\n")

    print(f"âœ… å·²ç”Ÿæˆæç¤ºè¯: {prompt_file}")

    # 4. ç”Ÿæˆåˆ†æç»Ÿè®¡
    stats = {
        'total_docs': len(docs),
        'valid_docs': len(all_texts),
        'total_terms': len(all_terms),
        'unique_terms': len(term_freq),
        'high_freq_terms': len([t for t, c in term_freq.items() if c >= 5]),
        'sample_file': str(sample_file),
        'freq_file': str(freq_file),
        'prompt_file': str(prompt_file)
    }

    stats_file = output_dir / 'extraction_stats.json'
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)

    print(f"âœ… å·²ä¿å­˜ç»Ÿè®¡: {stats_file}")

    print("\n" + "="*60)
    print("âœ… æå–å®Œæˆï¼")
    print("\nğŸ“‹ ä¸‹ä¸€æ­¥ï¼š")
    print(f"1. æŸ¥çœ‹æ ·æœ¬: cat {sample_file}")
    print(f"2. æŸ¥çœ‹è¯é¢‘: cat {freq_file}")
    print(f"3. å°† {prompt_file} çš„å†…å®¹å’Œæ ·æœ¬æ–‡ä»¶å‘é€ç»™ AI")
    print("4. AI ä¼šç”Ÿæˆä¼˜åŒ–çš„è¯å…¸")
    print("5. å°†ç”Ÿæˆçš„è¯å…¸åˆå¹¶åˆ° data/user_dict.txt")

if __name__ == '__main__':
    main()
